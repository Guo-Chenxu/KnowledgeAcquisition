# 信息检索系统 <!-- omit in toc -->

- [项目概述](#项目概述)
  - [项目要求](#项目要求)
  - [项目架构](#项目架构)
- [详细设计](#详细设计)
  - [数据爬取](#数据爬取)
    - [数据存储](#数据存储)
  - [前端设计与实现](#前端设计与实现)
  - [后端设计与实现](#后端设计与实现)
    - [图片搜索 \<!--- omit in toc ---\>](#图片搜索-----omit-in-toc----)
  - [信息检索服务](#信息检索服务)
  - [多媒体关键词提取服务](#多媒体关键词提取服务)
  - [检索结果评价](#检索结果评价)
- [优化与创新性](#优化与创新性)
- [环境和社会可持续发展思考](#环境和社会可持续发展思考)
- [实验总结](#实验总结)
- [实验分工](#实验分工)

## 项目概述

### 项目要求

本实验要求自己动手设计实现一个信息检索系统，中、英文皆可，数据源可以自选，数据通过开源的网络爬虫获取，规模不低于 100 篇文档，进行本地存储。中文可以分词（可用开源代码），也可以不分词，直接使用字作为基本单元。英文可以直接通过空格分隔。构建基本的倒排索引文件。实现基本的向量空间检索模型的匹配算法。用户查询输入可以是自然语言字串，查询结果输出按相关度从大到小排序，列出相关度、题目、主要匹配内容、URL、日期等信息。最好能对检索结果的准确率进行人工评价。界面不做强制要求，可以是命令行，也可以是可操作的界面。提交作业报告和源代码。

### 项目架构

本项目主要由四个部分组成：数据爬取、前端展示、后端处理数据和图片检索服务，整体架构图和详细内容如下：

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406080002101.png"/>

-   数据爬取使用 Scrapy 框架爬取网页文章并存储到 json 文件中，方便后续使用；
-   前端展示使用 Vue 框架实现，为用户提供了清晰直观的操作界面；
-   后端使用基于 Go 的 Gin 框架开发，接收用户请求并处理，并且对文章进行初始化处理，如：使用结巴框架进行分词、建立倒排索引、计算 TF-IDF 值等；
-   图片检索服务则使用 Python 进行开发，使用 Flask 框架给后端提供调用接口，使用 Tensorflow 框架对图片进行识别并提取关键词用于检索。

## 详细设计

### 数据爬取

本次实验要求不少于 100 篇文档，所以我们结合自己情况爬取了比较常用的全中文的[**OiWiki**](https://oi-wiki.org/)和最近在学习的全英文的[**The Rust Programming Language**](http://doc.rust-lang.org/book/)，最终爬取文章数量为中文 **440** 篇，英文 **104** 篇。爬虫框架选取了我们最为熟悉的 Scrapy，使用该框架可以快速爬取网页内容，并且可以方便的进行数据处理。

#### OiWiki 数据爬取 <!-- omit in toc -->

对于 OiWiki，我们首先爬取文章列表：

```python
def parse(self, response):
    sections = response.xpath(
        "//li[@class='md-nav__item']/a[@class='md-nav__link']"
    )
    hrefs = sections.xpath("@href").getall()
    texts = sections.xpath("text()").getall()
    texts = [t.strip() for t in texts]

    for href, section in zip(hrefs, texts):
        url = response.urljoin(href)
        yield scrapy.Request(
            url=url,
            callback=self.parse_section,
            cb_kwargs={"section": section},
        )
```

然后爬取每篇文章内容：

```python
def parse_section(self, response, section="Unknown"):
    content = response.xpath(
        '//div[@class="md-content"]//blockquote[1]/preceding-sibling::*[not(self::a)]'
    ).getall()
    keywords = response.xpath(
        '//div[@class="md-content"]//*[self::h1 or self::h2 or self::h3 or self::h4 or self::li or self::ul or self::p]/text()'
    ).getall()

    self.id = self.id + 1
    yield {
        "id": str(self.id),
        "title": content[0],
        "content": "".join(para for para in content),
        "keywords": "".join(para for para in keywords),
        "url": response.url,
        "date": datetime.date.today().strftime("%Y-%m-%d"),
    }
```

其中，我们将全文内容作为文章内容用于前端展示，文章中的所有文本内容作为关键字用于索引和检索。

#### The Rust Programming Language 数据爬取 <!-- omit in toc -->

对于 The Rust Programming Language，我们同样地也是先爬取文章所有章节，然后再爬取每个章节内的详细内容：

```python
def parse(self, response):
    chapters = response.xpath(
        '//ol[@class="chapter"]//li[@class="chapter-item expanded " or @class="chapter-item expanded affix "]/a'
    )
    hrefs = chapters.xpath("@href").getall()
    texts = chapters.xpath("text()").getall()

    for href, chapter in zip(hrefs, texts):
        url = response.urljoin(href)
        yield scrapy.Request(
            url=url,
            callback=self.parse_chapter,
            cb_kwargs={"chapter": chapter},
        )
def parse_chapter(self, response, chapter="Unknown"):
    content = response.xpath("//main/*")
    keywords = content.xpath("text()").getall()
    content = content.getall()

    self.id = self.id + 1
    yield {
        "id": str(self.id),
        "title": content[0],
        "content": "".join(p for p in content),
        "keywords": "".join(p for p in keywords),
        "url": response.url,
        "date": datetime.date.today().strftime("%Y-%m-%d"),
    }
```

#### 数据存储

为了方便后续索引和检索，我们将所有爬取到的数据都存储成`json`文件，每一条的数据的形式如下，保证所有要求的必要信息都会被存储：

```json
{
    "id": "1",
    "title": "The Rust Programming Language",
    "content": "The Rust Programming Language",
    "keywords": "The Rust Programming Language",
    "url": "https://doc.rust-lang.org/book/",
    "date": "2022-04-27"
}
```

最终的爬取结果如下：

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406081119362.png"/>

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406081119119.png"/>

### 前端设计与实现

todo hjr 写

### 后端设计与实现

因为 Go 在各种测试中表现出了优秀的性能水平，所以本次实验后端我使用 Go 语言进行开发，框架使用了 Gin 这一高性能且比较主流的 Web 框架。

不过 Go 令人最为诟病的一点就是其`err`的判断机制，几乎每一次函数调用都要判断函数返回的`err`是否为空，所以考虑到报告的篇幅长度，我在后续展示 Go 代码时都将删去`err`的判断部分，以便带来更加良好的阅读体验。

#### 接口展示 <!-- omit in toc -->

首先展示一下我们此次实验完成的所有接口（包括作业二和作业三）

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406081144216.png"/>

```go
r.GET("/swagger/*any", ginSwagger.WrapHandler(swaggerFiles.Handler))

v1 := r.Group("/api/v1")
{
	// Search with keywords
	v1.GET("/search", controller.Search)
	// Fetch SearchResult content details
	v1.GET("/document", controller.GetDocument)
	// Search by image
	v1.POST("/search_by_image", controller.SearchByImage)

	// Entities and hot words
	v1.GET("/extract_info", controller.ExtractInfo)
	// Entity and hot word feedback
	v1.POST("/extract_info_regex", controller.ExtractInfoRegex)

	// Feedback
	v1.POST("/feedback", controller.Feedback)
	// Entity Feedback
	v1.POST("/entity_feedback", controller.EntityFeedback)
	// Hotword Feedback
	v1.POST("/hotword_feedback", controller.HotwordFeedback)
	// Regex Feedback
	v1.POST("/extract_info_regex_feedback", controller.ExtractInfoRegexFeedback)
}
```

#### 关键词搜索 <!-- omit in toc -->

根据关键词时用户提交关键词，请求到达后端后，由 controller 调用对应的 logic 函数实现具体功能。具体实现过程是先查询缓存，缓存未命中则对关键词进行分词，然后调用核心的`SearchIndex`函数进行查询，`SearchIndex`函数的具体实现将会在[信息检索服务](#信息检索服务)中详细介绍，查询结束后则将结果放入缓存并向前端返回查询结果。

```go
func Search(q string, page string, resultsPerPage string) (r model.SearchResponse, err error) {
	cacheKey := fmt.Sprintf("%s-%s-%s", q, page, resultsPerPage)

	if cachedResults, found := cache.Get(cacheKey); found {
		return model.SearchResponse{Code: 200, Results: cachedResults}, nil
	}

	intPage, err := strconv.Atoi(page)

	intResultsPerPage, err := strconv.Atoi(resultsPerPage)

	queryWords := WordSplit(q)
	log.Info("queryWords: ", queryWords)

	results, err := SearchIndex(queryWords, intPage, intResultsPerPage)

	cache.Set(cacheKey, results)

	return model.SearchResponse{Code: 200, Results: results}, nil
}
```

#### 图片搜索 <!--- omit in toc --->

图片搜索需要调用 python 写的接口，通过模型识别从图片中提取关键字，然后使用关键词进行搜索。所以在 controller 中，图片搜索首先调用`GetKeywordsFromImage`函数，该函数会调用 python 接口提取图片关键词，python 的处理则会在[多媒体关键词提取服务](#多媒体关键词提取服务)中详细介绍。

```go
// GetKeywordsFromImage 调用python接口提取图片关键词
func GetKeywordsFromImage(imagePath string) (string, error) {
	var b bytes.Buffer
	w := multipart.NewWriter(&b)
	f, err := os.Open(imagePath)

	defer f.Close()

	fw, err := w.CreateFormFile("file", filepath.Base(imagePath))
	if _, err = io.Copy(fw, f); err != nil {
		return "", err
	}
	w.Close()

	req, err := http.NewRequest("POST", model.PYTHON_SERVER_URL+"/image_to_keywords", &b)
	req.Header.Set("Content-Type", w.FormDataContentType())

	client := &http.Client{}
	res, err := client.Do(req)

	if res.StatusCode != http.StatusOK {
		body, err := ioutil.ReadAll(res.Body)
		return "", errors.New(string(body))
	}

	body, err := ioutil.ReadAll(res.Body)

	var kr KeywordResponse
	err = json.Unmarshal(body, &kr)

	return kr.Keyword, nil
}
```

### 信息检索服务

### 多媒体关键词提取服务

### 检索结果评价

todo hjr 写, gcx 补充

## 优化与创新性

todo gcx 写, hjr 补充

## 环境和社会可持续发展思考

todo hjr 写

## 实验总结

## 实验分工

|      | 郭晨旭                                                                   | 韩景锐                                   |
| ---- | ------------------------------------------------------------------------ | ---------------------------------------- |
| 代码 | Go 后端，Scrapy 爬虫，Python 图片提取关键词服务                          | Vue 前端                                 |
| 报告 | 项目概述，后端设计与实现，信息检索服务，多媒体关键词提取服务，优化与创新 | 前端设计与实现，环境和社会可持续发展思考 |
