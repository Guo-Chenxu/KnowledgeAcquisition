# 信息检索系统 <!-- omit in toc -->

- [项目概述](#项目概述)
  - [项目要求](#项目要求)
  - [项目架构](#项目架构)
- [详细设计](#详细设计)
  - [数据爬取](#数据爬取)
    - [数据存储](#数据存储)
  - [前端设计与实现](#前端设计与实现)
  - [后端设计与实现](#后端设计与实现)
  - [信息检索系统设计与实现](#信息检索系统设计与实现)
  - [检索结果评价](#检索结果评价)
- [优化与创新性](#优化与创新性)
- [环境和社会可持续发展思考](#环境和社会可持续发展思考)
- [实验总结](#实验总结)
- [实验分工](#实验分工)

## 项目概述

### 项目要求

本实验要求自己动手设计实现一个信息检索系统，中、英文皆可，数据源可以自选，数据通过开源的网络爬虫获取，规模不低于 100 篇文档，进行本地存储。中文可以分词（可用开源代码），也可以不分词，直接使用字作为基本单元。英文可以直接通过空格分隔。构建基本的倒排索引文件。实现基本的向量空间检索模型的匹配算法。用户查询输入可以是自然语言字串，查询结果输出按相关度从大到小排序，列出相关度、题目、主要匹配内容、URL、日期等信息。最好能对检索结果的准确率进行人工评价。界面不做强制要求，可以是命令行，也可以是可操作的界面。提交作业报告和源代码。

### 项目架构

本项目主要由四个部分组成：数据爬取、前端展示、后端处理数据和图片检索服务，整体架构图和详细内容如下：

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406080002101.png"/>

-   数据爬取使用 Scrapy 框架爬取网页文章并存储到 json 文件中，方便后续使用；
-   前端展示使用 Vue 框架实现，为用户提供了清晰直观的操作界面；
-   后端使用基于 Go 的 Gin 框架开发，接收用户请求并处理，并且对文章进行初始化处理，如：使用结巴框架进行分词、建立倒排索引、计算 TF-IDF 值等；
-   图片检索服务则使用 Python 进行开发，使用 Flask 框架给后端提供调用接口，使用 Tensorflow 框架对图片进行识别并提取关键词用于检索。

## 详细设计

### 数据爬取

本次实验要求不少于 100 篇文档，所以我们结合自己情况爬取了比较常用的全中文的[**OiWiki**](https://oi-wiki.org/)和最近在学习的全英文的[**The Rust Programming Language**](http://doc.rust-lang.org/book/)，最终爬取文章数量为中文 **440** 篇，英文 **104** 篇。爬虫框架选取了我们最为熟悉的 Scrapy，使用该框架可以快速爬取网页内容，并且可以方便的进行数据处理。

#### OiWiki 数据爬取 <!-- omit in toc -->

对于 OiWiki，我们首先爬取文章列表：

```python
def parse(self, response):
    sections = response.xpath(
        "//li[@class='md-nav__item']/a[@class='md-nav__link']"
    )
    hrefs = sections.xpath("@href").getall()
    texts = sections.xpath("text()").getall()
    texts = [t.strip() for t in texts]

    for href, section in zip(hrefs, texts):
        url = response.urljoin(href)
        yield scrapy.Request(
            url=url,
            callback=self.parse_section,
            cb_kwargs={"section": section},
        )
```

然后爬取每篇文章内容：

```python
def parse_section(self, response, section="Unknown"):
    content = response.xpath(
        '//div[@class="md-content"]//blockquote[1]/preceding-sibling::*[not(self::a)]'
    ).getall()
    keywords = response.xpath(
        '//div[@class="md-content"]//*[self::h1 or self::h2 or self::h3 or self::h4 or self::li or self::ul or self::p]/text()'
    ).getall()

    self.id = self.id + 1
    yield {
        "id": str(self.id),
        "title": content[0],
        "content": "".join(para for para in content),
        "keywords": "".join(para for para in keywords),
        "url": response.url,
        "date": datetime.date.today().strftime("%Y-%m-%d"),
    }
```

其中，我们将全文内容作为文章内容用于前端展示，文章中的所有文本内容作为关键字用于索引和检索。

#### The Rust Programming Language 数据爬取 <!-- omit in toc -->

对于 The Rust Programming Language，我们同样地也是先爬取文章所有章节，然后再爬取每个章节内的详细内容：

```python
def parse(self, response):
    chapters = response.xpath(
        '//ol[@class="chapter"]//li[@class="chapter-item expanded " or @class="chapter-item expanded affix "]/a'
    )
    hrefs = chapters.xpath("@href").getall()
    texts = chapters.xpath("text()").getall()

    for href, chapter in zip(hrefs, texts):
        url = response.urljoin(href)
        yield scrapy.Request(
            url=url,
            callback=self.parse_chapter,
            cb_kwargs={"chapter": chapter},
        )
def parse_chapter(self, response, chapter="Unknown"):
    content = response.xpath("//main/*")
    keywords = content.xpath("text()").getall()
    content = content.getall()

    self.id = self.id + 1
    yield {
        "id": str(self.id),
        "title": content[0],
        "content": "".join(p for p in content),
        "keywords": "".join(p for p in keywords),
        "url": response.url,
        "date": datetime.date.today().strftime("%Y-%m-%d"),
    }
```

#### 数据存储

为了方便后续索引和检索，我们将所有爬取到的数据都存储成`json`文件，每一条的数据的形式如下，保证所有要求的必要信息都会被存储：

```json
{
    "id": "1",
    "title": "The Rust Programming Language",
    "content": "The Rust Programming Language",
    "keywords": "The Rust Programming Language",
    "url": "https://doc.rust-lang.org/book/",
    "date": "2022-04-27"
}
```

最终的爬取结果如下：

<!-- todo 截图 -->

### 前端设计与实现

todo hjr 写

### 后端设计与实现

### 信息检索系统设计与实现

### 检索结果评价

todo hjr 写, gcx 补充

## 优化与创新性

todo gcx 写, hjr 补充

## 环境和社会可持续发展思考

todo hjr 写

## 实验总结

## 实验分工

|      | 郭晨旭                                                       | 韩景锐                                   |
| ---- | ------------------------------------------------------------ | ---------------------------------------- |
| 代码 | go 后端，spider 爬虫，python 图片检索服务                    | vue 前端                                 |
| 报告 | 项目概述，后端设计与实现，信息检索系统设计与实现，优化与创新 | 前端设计与实现，环境和社会可持续发展思考 |
