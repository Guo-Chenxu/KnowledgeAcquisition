# 信息抽取系统 <!-- omit in toc -->

- [项目概述](#项目概述)
  - [项目要求](#项目要求)
  - [项目架构](#项目架构)
- [详细设计](#详细设计)
  - [数据爬取](#数据爬取)
    - [OiWiki 数据爬取](#oiwiki-数据爬取)
    - [The Rust Programming Language 数据爬取](#the-rust-programming-language-数据爬取)
    - [数据存储](#数据存储)
  - [后端设计与实现](#后端设计与实现)
    - [接口展示](#接口展示)
    - [默认信息提取](#默认信息提取)
    - [正则和信息点信息提取](#正则和信息点信息提取)
  - [前端设计与实现](#前端设计与实现)
    - [框架选择与设计理念](#框架选择与设计理念)
    - [界面设计与接口实现](#界面设计与接口实现)
    - [关键词搜索](#关键词搜索)
    - [图片检索](#图片检索)
    - [搜索结果展示](#搜索结果展示)
    - [搜索结果反馈](#搜索结果反馈)
      - [实体反馈](#实体反馈)
      - [热词反馈](#热词反馈)
      - [整体准确率评价反馈](#整体准确率评价反馈)
  - [信息抽取服务](#信息抽取服务)
    - [热词和实体抽取](#热词和实体抽取)
    - [正则和指定信息点抽取](#正则和指定信息点抽取)
    - [实体词检测和提取](#实体词检测和提取)
  - [多媒体信息抽取服务](#多媒体信息抽取服务)
  - [抽取结果评价](#抽取结果评价)
    - [实体反馈](#实体反馈-1)
    - [热词反馈](#热词反馈-1)
    - [整体准确率评价反馈](#整体准确率评价反馈-1)
- [优化与创新性](#优化与创新性)
  - [后端-算法优化](#后端-算法优化)
  - [前端](#前端)
- [环境和社会可持续发展思考](#环境和社会可持续发展思考)
- [实验总结](#实验总结)
- [实验分工](#实验分工)

<div STYLE="page-break-after: always;"></div>

## 项目概述

### 项目要求

基本要求：自己动手设计实现一个信息抽取实验系统，中、英文皆可，可以在作业 2 信息检索系统的基础上实现，也可以单独实现。特定领域语料根据自己的兴趣选定，规模不低于 100 篇文档，进行本地存储。对自己感兴趣的特定信息点进行抽取，并将结果展示出来。其中，特定信息点的个数不低于 5 个。可以调用开源的中英文自然语言处理基本模块，如分句、分词、命名实体识别、句法分析。信息抽取算法可以根据自己的兴趣选择，至少实现正则表达式匹配算法的特定信息点抽取。最好能对抽取结果的准确率进行人工评价。界面不作强制要求，可以是命令行，也可以是可操作的界面。提交作业报告和源代码。鼓励有兴趣和有能力的同学积极尝试优化各模块算法，也可关注各类相关竞赛。

扩展要求：鼓励有兴趣和有能力的同学积极尝试多媒体信息抽取以及优化各模块算法，也可关注各类相关竞赛。自主开展相关文献调研与分析，完成算法评估、优化、论证创新点的过程。

### 项目架构

本次实验在第二次实验的基础上完成，除了数据爬取、前端展示、后端处理和图片检索服务外还而额外增加了**信息抽取服务**，整体架构图和详细内容如下：

<!-- todo 修改 -->
<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406080002101.png"/>

-   数据爬取使用 Scrapy 框架爬取网页文章并存储到 json 文件中，方便后续使用；
-   前端展示使用 Vue 框架实现，为用户提供了清晰直观的操作界面；
-   后端使用基于 Go 的 Gin 框架开发，接收用户请求并处理，并且对文章进行初始化处理，如：使用结巴框架进行分词、建立倒排索引、计算 TF-IDF 值等；
-   图片检索服务则使用 Python 进行开发，使用 Flask 框架给后端提供调用接口，使用 Tensorflow 框架对图片进行识别并提取关键词用于检索。
-   信息抽取服务也是使用 Python 进行开发，针对中英文两种语言分别使用 jieba 和 spaCy 两种框架进行处理和抽取。

项目运行截图如下：

前端界面展示：

<!-- todo 前端添加图片 -->

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406082233411.png"/>

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406082233052.png"/>

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406082228601.png"/>

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406082229972.png"/>

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406082242547.png"/>

后端日志展示：

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406082152454.png"/>

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406082212933.png"/>

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406082212062.png"/>

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406271753793.png"/>

## 详细设计

### 数据爬取

本次实验要求不少于 100 篇文档，所以我们结合自己情况爬取了比较常用的全中文的[**OiWiki**](https://oi-wiki.org/)和最近在学习的全英文的[**The Rust Programming Language**](http://doc.rust-lang.org/book/)，最终爬取文章数量为中文 **440** 篇，英文 **104** 篇。爬虫框架选取了我们最为熟悉的 Scrapy，使用该框架可以快速爬取网页内容，并且可以方便的进行数据处理。

#### OiWiki 数据爬取

对于 OiWiki，我们首先爬取文章列表：

```python
def parse(self, response):
    sections = response.xpath(
        "//li[@class='md-nav__item']/a[@class='md-nav__link']"
    )
    hrefs = sections.xpath("@href").getall()
    texts = sections.xpath("text()").getall()
    texts = [t.strip() for t in texts]

    for href, section in zip(hrefs, texts):
        url = response.urljoin(href)
        yield scrapy.Request(
            url=url,
            callback=self.parse_section,
            cb_kwargs={"section": section},
        )
```

然后爬取每篇文章内容：

```python
def parse_section(self, response, section="Unknown"):
    content = response.xpath(
        '//div[@class="md-content"]//blockquote[1]/preceding-sibling::*[not(self::a)]'
    ).getall()
    keywords = response.xpath(
        '//div[@class="md-content"]//*[self::h1 or self::h2 or self::h3 or self::h4 or self::li or self::ul or self::p]/text()'
    ).getall()

    self.id = self.id + 1
    yield {
        "id": str(self.id),
        "title": content[0],
        "content": "".join(para for para in content),
        "keywords": "".join(para for para in keywords),
        "url": response.url,
        "date": datetime.date.today().strftime("%Y-%m-%d"),
    }
```

其中，我们将全文内容作为文章内容用于前端展示，文章中的所有文本内容作为关键字用于索引和检索。

#### The Rust Programming Language 数据爬取

对于 The Rust Programming Language，我们同样地也是先爬取文章所有章节，然后再爬取每个章节内的详细内容：

```python
def parse(self, response):
    chapters = response.xpath(
        '//ol[@class="chapter"]//li[@class="chapter-item expanded " or @class="chapter-item expanded affix "]/a'
    )
    hrefs = chapters.xpath("@href").getall()
    texts = chapters.xpath("text()").getall()

    for href, chapter in zip(hrefs, texts):
        url = response.urljoin(href)
        yield scrapy.Request(
            url=url,
            callback=self.parse_chapter,
            cb_kwargs={"chapter": chapter},
        )
def parse_chapter(self, response, chapter="Unknown"):
    content = response.xpath("//main/*")
    keywords = content.xpath("text()").getall()
    content = content.getall()

    self.id = self.id + 1
    yield {
        "id": str(self.id),
        "title": content[0],
        "content": "".join(p for p in content),
        "keywords": "".join(p for p in keywords),
        "url": response.url,
        "date": datetime.date.today().strftime("%Y-%m-%d"),
    }
```

#### 数据存储

为了方便后续索引和检索，我们将所有爬取到的数据都存储成`json`文件，每一条的数据的形式如下，保证所有要求的必要信息都会被存储：

```json
{
    "id": "1",
    "title": "The Rust Programming Language",
    "content": "The Rust Programming Language",
    "keywords": "The Rust Programming Language",
    "url": "https://doc.rust-lang.org/book/",
    "date": "2022-04-27"
}
```

最终的爬取结果如下：

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406081119362.png"/>

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406081119119.png"/>

### 后端设计与实现

因为 Go 在各种测试中表现出了优秀的性能水平，所以本次实验后端我使用 Go 语言进行开发，框架使用了 Gin 这一高性能且比较主流的 Web 框架，本部分将介绍本项目的后端接口设计以及相关逻辑实现。

不过 Go 令人最为诟病的一点就是其`err`的判断机制，几乎每一次函数调用都要判断函数返回的`err`是否需要处理，所以考虑到报告的篇幅长度，我在后续展示 Go 代码时都将删去`err`的判断部分，以便带来更加良好的阅读体验。

#### 接口展示

首先展示一下我们此次实验完成的所有接口（包括作业二和作业三）

<img src="https://cdn.jsdelivr.net/gh/Guo-Chenxu/imgs@main/imgs/202406081144216.png"/>

```go
r.GET("/swagger/*any", ginSwagger.WrapHandler(swaggerFiles.Handler))

v1 := r.Group("/api/v1")
{
	// Search with keywords
	v1.GET("/search", controller.Search)
	// Fetch SearchResult content details
	v1.GET("/document", controller.GetDocument)
	// Search by image
	v1.POST("/search_by_image", controller.SearchByImage)

	// Entities and hot words
	v1.GET("/extract_info", controller.ExtractInfo)
	// Entity and hot word feedback
	v1.POST("/extract_info_regex", controller.ExtractInfoRegex)

	// Feedback
	v1.POST("/feedback", controller.Feedback)
	// Entity Feedback
	v1.POST("/entity_feedback", controller.EntityFeedback)
	// Hotword Feedback
	v1.POST("/hotword_feedback", controller.HotwordFeedback)
	// Regex Feedback
	v1.POST("/extract_info_regex_feedback", controller.ExtractInfoRegexFeedback)
}
```

#### 默认信息提取

默认每篇文章我们会提取出该篇文章的热词和部分实体进行展示，在执行过程中我收到请求后在后端查询该篇文章详细内容，然后将其作为参数发送给 Python 接口，具体提取逻辑见[热词和实体抽取](#热词和实体抽取)。

后端代码如下：

```go
func ExtractInfo(doc_id string) (model.DocumentAbstract, error) {
	var result model.DocumentAbstract

	doc, ok := idDocMap[doc_id]
	if !ok {
		log.Error("Error getting doc ", doc_id)
	}
	data := map[string]string{"text": doc.Keywords, "language": doc.Lang.String()}
	jsonData, _ := json.Marshal(data)

	resp, err := http.Post(model.PYTHON_SERVER_URL+"/extract_info", "application/json", bytes.NewBuffer(jsonData))
	if err != nil {
		return model.DocumentAbstract{}, err
	}
	defer resp.Body.Close()

	json.NewDecoder(resp.Body).Decode(&result)

	if resp.StatusCode != http.StatusOK {
		body, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			log.Error(err.Error())
			return model.DocumentAbstract{}, err
		}
		return model.DocumentAbstract{}, errors.New(string(body))
	}

	log.Debug("Extract info for doc ", doc_id, " entities: ", result.Entities, " hot_words: ", result.HotWords)

	return result, nil
}
```

#### 正则和信息点信息提取

正则和信息点抽取主要是我们可以让用户自定义**正则表达式**和想要抽取的**信息点**（如：人名、法律、序数词、日期、量词、地理位置、产品名以及组织名等），后端接收到参数后也是先查询该篇文章详细内容，然后将其作为参数发送给 Python 接口，具体提取逻辑见[正则和指定信息点抽取](#正则和指定信息点抽取)。

后端代码如下：

```go
func ExtractInfoRegex(doc_id, pattern, word_class string) (model.DocumentExtractRegex, error) {
	var result model.DocumentExtractRegex

	doc, ok := idDocMap[doc_id]
	if !ok {
		log.Error("Error getting doc ", doc_id)
	}
	data := map[string]string{"text": doc.Keywords, "language": doc.Lang.String(), "pattern": pattern, "word_class": word_class}
	jsonData, _ := json.Marshal(data)

	resp, err := http.Post(model.PYTHON_SERVER_URL+"/extract_info_regex", "application/json", bytes.NewBuffer(jsonData))
	if err != nil {
		return model.DocumentExtractRegex{}, err
	}
	defer resp.Body.Close()

	json.NewDecoder(resp.Body).Decode(&result)

	if resp.StatusCode != http.StatusOK {
		body, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			log.Error(err.Error())
			return model.DocumentExtractRegex{}, err
		}
		return model.DocumentExtractRegex{}, errors.New(string(body))
	}

	log.Debug("Extract info for doc ", doc_id, " pattern: ", pattern, " word_class: ", word_class, " words: ", result.Words)

	return result, nil
}
```

搜索功能也是本系统必不可少的一部分，但由于篇幅限制就不再赘述，详情请见[实验二报告](./docs/作业2-郭晨旭-韩景锐.pdf)

### 前端设计与实现

#### 框架选择与设计理念

我们选择 **Vue.js** 作为前端开发的框架，并结合 **Vuetify** 组件库，符合现代网页设计趋势，又能提供丰富的交互元素来增强用户体验。

本项目的前端界面设计旨在提供清晰、直观的用户交互体验。通过简洁的设计风格和直观的操作流程，用户能够轻松进行信息检索，无论是通过关键词搜索还是图片检索。设计重点放在用户操作的便捷性和界面的响应速度上，以适应不同背景知识的用户。

#### 界面设计与接口实现

**初始界面：**

![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609155511735.png)

**顶栏与 Logo**：

-   **组件**：`<v-img>`
-   **功能**：在页面顶部显示顶栏，正文顶部中央展示系统的 Logo。
-   ![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609155149992.png)

#### 关键词搜索

-   ![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609155130296.png)

-   **组件**：`<v-text-field>` 和 `<v-btn>`

-   **功能**：

    -   `v-text-field`：允许用户输入搜索关键词，可以有多个关键词，关键词之间空格隔开。
    -   `v-btn`：点击后触发搜索功能，按钮文本为“搜索”，颜色为主题色，增加视觉效果。

-   **布局**：关键词输入框和搜索按钮在同一行显示，使用 Vuetify 的布局系统优化空间利用。

-   **接口函数：**

    ```javascript
    searchByKeyword() {
       const params = { q: this.searchText };
       axios
         .get(`api/v1/search`, { params })
         .then((response) => {
            this.searchResults = response.data;
         })
         .catch((error) => {
            console.error("Error during keyword search:", error);
         });
    },
    ```

#### 图片检索

-   ![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609161432057.png)

-   **组件**：`<v-file-input>` 和 `<v-btn>`

-   **功能**：

    -   `v-file-input`：提供图片上传功能，标签“点击上传图片”指导用户操作，支持文件类型过滤。

    -   `v-btn`：用于触发图片的上传和检索处理，保持与关键词搜索按钮一致的风格。

        点击搜索按钮后，后端将识别图片并返回与输入图片相关的关键词显示在关键词搜索框中。

-   **布局**：与关键词搜索类似，确保操作的一致性和界面的整洁。

-   **接口函数：**

    ```javascript
    searchByImage() {
      if (!this.imageFile) {
        alert("Please upload an image.");
        return;
      }
      const formData = new FormData();
      formData.append("image", this.imageFile);
      axios
        .post(`api/v1/search_by_image`, formData, {
          headers: { "Content-Type": "multipart/form-data" },
        })
        .then((response) => {
          this.searchResults = response.data.results;
          this.searchText = response.data.keywords;
        })
        .catch((error) => {
          console.error("Error during image search:", error);
        });
    },
    ```

#### 搜索结果展示

![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609160902466.png)

-   **组件**：`<v-card>`

-   **功能**：

    -   动态展示搜索结果，每个结果为一个卡片，展示包括**文档标题、相关度、介绍信息、链接**（可点击跳转），**右下角**有对每个搜索结果的**评分反馈**，用户可以进行评分。
    -   用户可点击每张卡片的标题，**点击标题将根据该结果的文章 id，从后端调取本结果的详细信息**。（再次点击即可隐藏）
    -   详细信息包括：**整篇文章的所有内容、文章关键字、文章实体表格、文章热词表格**。
    -   ![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609162043038.png)
    -   ![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609162126049.png)

-   **布局**：结果以列表形式排列，通过 Vuetify 的响应式布局确保在不同设备上的显示效果。

-   **接口函数**：获取文档详细信息和实体热词表格。

    ```javascript
    toggleDetail(id) {
      // 检查detailMap中是否存在该id且其visible属性为true
      if (this.detailMap[id] && this.detailMap[id].visible) {
        // 如果已经可见，则设置为不可见
        this.$set(this.detailMap[id], "visible", false);
      }
      // 检查detailMap中是否存在该id且其visible属性为false
      else if (this.detailMap[id] && !this.detailMap[id].visible) {
        // 如果不可见，则设置为可见
        this.$set(this.detailMap[id], "visible", true);
      } else {
        // 如果detailMap中没有该id的信息，通过axios请求获取数据
        axios
          .all([
            axios.get(`api/v1/document`, { params: { id } }),  // 请求文档详情
            axios.get(`api/v1/extract_info`, { params: { id } }),  // 请求提取信息
          ])
          .then(
            axios.spread((DocRes, infoRes) => {
              // 处理响应数据
              const entitiesWithScore = Object.entries(infoRes.data.entities).reduce((acc, [key, value]) => {
                // 初始化每个实体的评分为0
                acc[key] = { value, score: 0 };
                return acc;
              }, {});

              const hotWordsWithScore = Object.entries(infoRes.data.hot_words).reduce((acc, [key, value]) => {
                // 初始化每个热词的评分为0
                acc[key] = { value, score: 0 };
                return acc;
              }, {});

              // 设置detailMap以包含获取的详情数据
              this.$set(this.detailMap, id, {
                visible: true,  // 设置为可见
                content: DocRes.data.content,  // 文档内容
                keywords: DocRes.data.keywords,  // 关键词
                Lang: DocRes.data.Lang,  // 语言
                entities: infoRes.data.entities,  // 实体
                hot_words: infoRes.data.hot_words,  // 热词
              });
            })
          )
          .catch((error) => {
            // 处理请求错误
            console.error("Error fetching Document details:", error);
          });
      }
    }

    ```

#### 搜索结果反馈

##### 实体反馈

-   **组件**：`<v-rating>`

    ```html
    <v-rating
        dense
        hover
        small
        v-model="detailMap[result.Doc.id].entities[key].score"
        @input="handleEntityFeedback(result.Doc.id, key, detailMap[result.Doc.id].entities[key].score)"
    ></v-rating>
    ```

    ![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609171828993.png)

-   **功能**：允许用户对搜索结果中每个实体的准确性进行评分。

-   **属性**：

    -   `dense` 和 `small` 使评分组件更紧凑、适合放置在搜索结果卡片中。
    -   `hover` 允许用户在鼠标悬停时预览评分效果。
    -   `v-model` 绑定到 `detailMap[result.Doc.id].entities[key].score`，实现数据的双向绑定。
    -   `@input` 事件处理函数 `handleEntityFeedback` 发送用户的评分到后端。

-   **布局**：每个搜索结果的详细信息区域均设有实体反馈评分组件，与实体信息并排展示。

-   **接口函数：**

    ```js
    handleEntityFeedback(resultId, item, score) {
      const payload = {
        item,
        resultId,
        score
      };
      axios.post(`api/v1/entity_feedback`, payload)
        .then(response => {
          console.log("Entity Feedback sent successfully", response);
        })
        .catch(error => {
          console.error("Error sending entity feedback", error);
        });
    },
    ```

##### 热词反馈

-   **组件**：`<v-rating>`

    ```html
    <v-rating
        dense
        hover
        small
        v-model="detailMap[result.Doc.id].hot_words[key].score"
        @input="handleHotwordFeedback(result.Doc.id, key, detailMap[result.Doc.id].hot_words[key].score)"
    ></v-rating>
    ```

    ![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609171801354.png)

-   **功能**：允许用户对搜索结果中的热词进行评分。

-   **属性**：

    -   `dense` 和 `small` 使评分组件更紧凑、适合放置在搜索结果卡片中。
    -   `hover` 允许用户在鼠标悬停时预览评分效果。
    -   `v-model` 绑定到 `detailMap[result.Doc.id].hot_words[key].score`，实现数据的双向绑定。
    -   `@input` 事件处理函数 `handleEntityFeedback` 发送用户的评分到后端。

-   **布局**：与实体反馈类似，热词反馈组件与对应的热词信息并排展示。

-   **接口函数**：

    ```js
    handleHotwordFeedback(resultId, item, score) {
      const payload = {
        item,
        resultId,
        score
      };
      axios.post(`api/v1/hotword_feedback`, payload)
        .then(response => {
          console.log("Hotword Feedback sent successfully", response);
        })
        .catch(error => {
          console.error("Error sending hotword feedback", error);
        });
    },
    ```

##### 整体准确率评价反馈

-   **组件**：`<v-rating>`

    ```html
    <v-rating
        dense
        hover
        v-model="result.Score"
        @input="handleOverallFeedback(result.Doc.id, result.Score)"
    ></v-rating>
    ```

    ![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609171734666.png)

-   **功能**：提供对整个搜索结果的整体准确率满意度评价。

-   **属性**：

    -   `hover` 和 `dense` 属性同上。
    -   `v-model` 绑定到 `result.Score`。
    -   `@input` 通过 `handleOverallFeedback` 方法发送整体评分数据到后端。

-   **布局**：整体评价组件位于搜索结果卡片的底部，方便用户在查看完信息后给出整体准确率评价。

-   **接口函数**：

    ```js
    handleOverallFeedback(resultId, Score) {
      const payload = {
        resultId,
        Score
      };
      axios.post(`api/v1/feedback`, payload)
        .then(response => {
          console.log("Overall Feedback sent successfully", response);
        })
        .catch(error => {
          console.error("Error sending overall feedback", error);
        });
    }
    ```

### 信息抽取服务

信息检索服务是本次实验的核心内容，在实验中，我完成了三种情况的信息提取——用户自定义**正则表达式和信息点**提取、部分实体词以及高频词提取。其中用户自定义的信息点总体上有人名、法律、序数词、日期、量词、地理位置、产品名以及组织名这**七种**。在实际实验中，这部分的实际逻辑主要由 python 服务完成，go 服务中处理处理请求数据，然后调用 python 服务进行分词处理和提取。

#### 热词和实体抽取

该部分是先接受`/extract_info`路径的请求，然后调用`entity_detect`函数通过不同的处理方式提取出所有实体，然后选取频率前五的实体词作为热词返回，具体的抽取逻辑`entity_dectect`见[实体词检测和抽取](#实体词检测和提取)，下面展示路由的处理和热词提取的处理代码：

```python
@app.route("/extract_info", methods=["POST"])
def extract_info():
    data = request.get_json()
    text = data.get("text")
    language = data.get("language")
    log.info("Data language: " + language)

    if not text or not language:
        return "Invalid request: no text or no language", 400

    if language not in ["en", "cn"]:
        return "Unsupported language: " + language, 400

    # Entity detection
    entities = entity_detection.entity_detect(text, language)

    # Extract hot words
    stop_words = (
        set(stopwords.words("english")) if language == "en" else set()
    )
    word_tokens = word_tokenize(text)
    words = [
        w for w in word_tokens if not w in stop_words and not w in punctuation
    ]

    hot_words = dict(Counter(words).most_common(5))

    entities = [e["text"] for e in entities]
    entities = dict(Counter(entities).most_common(5))
    entities = {k: v for k, v in entities.items() if v > 1}

    jsonResponse = json.dumps({"entities": entities, "hot_words": hot_words})
    log.debug(jsonResponse)
    return jsonResponse
```

#### 正则和指定信息点抽取

在本次实验中我们提供了**人名、法律、序数词、日期、量词、地理位置、产品名、组织名**这七种信息点可以选择抽取，抽取的过程如下：

1. 检测实体并提取；
2. 对每个词语判断是否匹配正则表达式和符合规定信息点；
3. 组装并返回结果

具体代码如下：

```python
@app.route("/extract_info_regex", methods=["POST"])
def extract_info_regex():
    data = request.get_json()
    text = data.get("text")
    pattern = data.get("pattern")
    language = data.get("language")
    word_class = data.get("word_class")

    if not text or not language:
        return "Invalid request: no text or no language", 400

    if language not in ["en", "cn"]:
        return "Unsupported language: " + language, 400

    # Entity detection
    entities = entity_detection.entity_detect(text, language)

    # Extract words with regex
    words = []
    entities = [{"text": item["text"], "label": item["label"]} for item in entities]
    for entity in entities:
        if word_class == entity.get("label") and bool(re.fullmatch(pattern=pattern,string=entity.get('text'))):
            words.append(entity.get('text'))

    jsonResponse = json.dumps({"words": words})
    log.debug(jsonResponse)
    return jsonResponse
```

#### 实体词检测和提取

如何从文章中抽取出实体词，这是此次实验的核心问题，由于有中英文两种不同的文本，所以在实验中我也采用了两种不同的方法分别进行处理：

- 对于中文文本：jieba处理库对于中文文本已经有了非常好的处理和检测能力，它对于每个词都能准确地分割并识别其词性，所以我使用jieba库对中文文本进行处理；
- 对于英文文本：对于英文文本我使用了spaCy框架，该库可以使用模型对文本进行分词和词性标注，提高了结果的准确率。在模型方面，我选择了`en_core_web_sm`模型。

具体我的处理代码如下：

```python
def entity_detect(text: str, language: str) -> str:
    entities = ""
    if language == "en":
        entities = en_entity_detect(text)
    elif language == "cn":
        entities = cn_entity_detect(text)
    print(f"entities: {entities}")
    return entities


def en_entity_detect(text: str) -> str:
    entities = []
    doc = nlp_en(text)
    # Extract entities
    for entity in doc.ents:
        entities.append(
            {
                "text": entity.text,
                "label": entity.label_,
            }
        )

    return entities


def cn_entity_detect(text: str) -> str:
    entities = []
    words = pseg.cut(text)
    # Extract entities
    for word, flag in words:
        entities.append({"text": word, "label": flag})

    return entities
```

### 多媒体信息抽取服务

想要实现多媒体的信息抽取，首先就要解决从多媒体中提取出关键词的问题，我在这里沿用了实验二的代码进行抽取关键词信息，并将其和实验三中[信息抽取服务](#信息抽取服务)进行融合。

具体流程是：

1. python 使用 flask 框架接受请求；
2. 将图片输入到 ResNet50 模型中进行对象识别；
3. 返回识别到的关键词；
4. 调用信息抽取的代码，使用识别到的关键词进行抽取。

接口代码如下：

```python
@app.route("/image_to_keywords", methods=["POST"])
def image_to_keywords():
    if "file" not in request.files:
        return "No file part", 400
    file = request.files["file"]

    if file.filename == "":
        return "No selected file", 400
    log.info(file.filename)
    result, code = image_detection.image_to_keywords(file)
    if code != 200:
        log.error(result)
        return result, code
    return result, code
```

模型处理代码如下（错误处理代码则删去不再展示）：

```python
config = tf.compat.v1.ConfigProto(
    gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))
sess = tf.compat.v1.Session(config=config)

log = logging.getLogger("ImageToKeywords")

# Image object detection model
model = ResNet50(weights="imagenet")


def image_to_keywords(file: str) -> tuple[str, int]:
    if file.filename == "":
        return ("No selected file", 400)
    log.info(file.filename)

    img = (
        Image.open(io.BytesIO(file.read()))
        .convert("RGB")
        .resize((224, 224))
    )

    x = img_to_array(img)

    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)

    preds = model.predict(x)
    predictions = decode_predictions(preds, top=5)[0]

    if len(predictions) >= 3:
        keywords = [pred[1] for pred in predictions[:3]]
    else:
        keywords = [pred[1] for pred in predictions]
    keywords = " ".join(kw for kw in keywords)
    log.info(keywords)
    return (json.dumps({"keyword": keywords}), 200)
```

### 抽取结果评价

<!-- todo 正则抽取评价 -->

#### 实体反馈

-   **组件**：`<v-rating>`

    ```html
    <v-rating
        dense
        hover
        small
        v-model="detailMap[result.Doc.id].entities[key].score"
        @input="handleEntityFeedback(result.Doc.id, key, detailMap[result.Doc.id].entities[key].score)"
    ></v-rating>
    ```

    ![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609171828993.png)

-   **功能**：允许用户对搜索结果中每个实体的准确性进行评分。

-   **属性**：

    -   `dense` 和 `small` 使评分组件更紧凑、适合放置在搜索结果卡片中。
    -   `hover` 允许用户在鼠标悬停时预览评分效果。
    -   `v-model` 绑定到 `detailMap[result.Doc.id].entities[key].score`，实现数据的双向绑定。
    -   `@input` 事件处理函数 `handleEntityFeedback` 发送用户的评分到后端。

-   **布局**：每个搜索结果的详细信息区域均设有实体反馈评分组件，与实体信息并排展示。

-   **接口函数：**

    ```js
    handleEntityFeedback(resultId, item, score) {
      const payload = {
        item,
        resultId,
        score
      };
      axios.post(`api/v1/entity_feedback`, payload)
        .then(response => {
          console.log("Entity Feedback sent successfully", response);
        })
        .catch(error => {
          console.error("Error sending entity feedback", error);
        });
    },
    ```

#### 热词反馈

-   **组件**：`<v-rating>`

    ```html
    <v-rating
        dense
        hover
        small
        v-model="detailMap[result.Doc.id].hot_words[key].score"
        @input="handleHotwordFeedback(result.Doc.id, key, detailMap[result.Doc.id].hot_words[key].score)"
    ></v-rating>
    ```

    ![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609171801354.png)

-   **功能**：允许用户对搜索结果中的热词进行评分。

-   **属性**：

    -   `dense` 和 `small` 使评分组件更紧凑、适合放置在搜索结果卡片中。
    -   `hover` 允许用户在鼠标悬停时预览评分效果。
    -   `v-model` 绑定到 `detailMap[result.Doc.id].hot_words[key].score`，实现数据的双向绑定。
    -   `@input` 事件处理函数 `handleEntityFeedback` 发送用户的评分到后端。

-   **布局**：与实体反馈类似，热词反馈组件与对应的热词信息并排展示。

-   **接口函数**：

    ```js
    handleHotwordFeedback(resultId, item, score) {
      const payload = {
        item,
        resultId,
        score
      };
      axios.post(`api/v1/hotword_feedback`, payload)
        .then(response => {
          console.log("Hotword Feedback sent successfully", response);
        })
        .catch(error => {
          console.error("Error sending hotword feedback", error);
        });
    },
    ```

#### 整体准确率评价反馈

-   **组件**：`<v-rating>`

    ```html
    <v-rating
        dense
        hover
        v-model="result.Score"
        @input="handleOverallFeedback(result.Doc.id, result.Score)"
    ></v-rating>
    ```

    ![](https://cdn.jsdelivr.net/gh/NonEspoir/figures@main/img/image-20240609171734666.png)

-   **功能**：提供对整个搜索结果的整体准确率满意度评价。

-   **属性**：

    -   `hover` 和 `dense` 属性同上。
    -   `v-model` 绑定到 `result.Score`。
    -   `@input` 通过 `handleOverallFeedback` 方法发送整体评分数据到后端。

-   **布局**：整体评价组件通常位于搜索结果卡片的底部，方便用户在查看完信息后给出整体准确率评价。

-   **接口函数**：

    ```js
    handleOverallFeedback(resultId, Score) {
      const payload = {
        resultId,
        Score
      };
      axios.post(`api/v1/feedback`, payload)
        .then(response => {
          console.log("Overall Feedback sent successfully", response);
        })
        .catch(error => {
          console.error("Error sending overall feedback", error);
        });
    }
    ```

## 优化与创新性

在本次实验中，通过不断地打磨我们的项目，我们实现了以下优化和创新：

### 后端-算法优化

-   缓存优化：后端自己实现了`LRU`缓存，在内存中缓存每次查询的结果，减少了重复的查询计算，提高了查询的性能和效率；
-   并发优化：在检索时我通过 Go 的协程并行计算每篇文档的得分，这样充分利用了 Go 轻量级协程的优势，大大提高了检索的性能；
-   抽取优化：在抽取过程中，我分别使用了 spaCy 和 jieba 对中英文不同文本进行分词和词性识别处理，大大提高了抽取的准确性和效率；
-   基于用户反馈动态修改排名：对于每个查询结果，我们都设置用户可以对其进行反馈，并且通过用户反馈修改查询结果的权重，动态地调整查询结果的排名；
-   多媒体抽取：在项目中我们引入了图片识别模型，通过识别提取图片关键词，从而实现多媒体检索和信息抽取。

### 前端

**优化措施**

-   **异步数据处理**：使用 Vue.js 的异步组件和 `axios` 进行数据请求，优化了页面加载速度和响应时间，避免了在请求数据时阻塞用户界面的问题。
-   **组件化开发**：利用 Vue.js 的组件化能力，将前端界面分解为可重用的组件，如搜索栏、结果卡片、反馈评分等。这种方式不仅提高了代码的可维护性，也简化了功能的扩展。
-   **用户体验优化**：
    -   在关键组件中实施了响应式设计，确保在不同设备上都能提供良好的用户体验。
    -   通过细致的动画和过渡效果增强了界面的交互性，如加载动画和按钮点击反馈。
    -   针对用户操作提供即时的反馈信息，比如在发送反馈评分后，通过控制台日志确认反馈已成功发送。
-   **错误处理与数据验证**：
    -   在数据输入和网络请求中实施了全面的错误处理机制，确保了应用的稳定运行和用户的流畅体验。
    -   对用户输入进行验证，防止无效或恶意的数据被提交到后端。

**创新性特点**

-   **集成的反馈机制**：引入了多层次的用户反馈系统，包括实体评分、热词评分和整体搜索结果评分。这不仅使用户能够直接参与改善搜索结果，也为算法优化提供了实时数据。
-   **多模态搜索功能**：实现了关键词搜索与图像搜索的结合，提供了一种多模态的信息检索方式。用户可以通过文本或图像中的内容进行搜索，增强了搜索的灵活性和准确性。
-   **实时动态更新**：通过 Vue.js 的双向数据绑定和组件状态管理，实现了搜索结果和用户反馈的实时动态更新。用户在界面上的任何操作都可以即时反映，提高了交互的实时性。

## 环境和社会可持续发展思考

**环境影响**

-   **减少能源消耗**：信息检索系统通过自动化处理大量数据，可能会对服务器造成较大负荷，进而影响能源消耗。为了降低这种影响，我们采用了能效较高的数据处理算法和节能的服务器配置。此外，通过优化爬虫的爬取策略，减少不必要的数据请求，可以进一步减轻对源网站服务器的压力，间接减少整个网络中的能源消耗。

-   **绿色技术选择**：在选择第三方服务和云平台时，优先考虑那些承诺使用可再生能源并具有良好碳足迹记录的供应商。例如，使用支持绿色能源的数据中心可以减少项目的环境负担。

**社会效益**

-   **信息获取的公平性**：本信息检索系统支持中英文内容的处理，有助于跨语言和文化的信息流通，促进了知识的平等获取。这对于教育资源的公平分配尤其重要，可以帮助来自不同背景的用户访问和利用全球的信息资源。

-   **促进知识共享和教育**：系统提供的多媒体关键词提取服务和自然语言查询功能，使用户能够更容易地找到所需信息，从而促进知识的传播和教育的普及。这种技术的应用尤其可以支持教育不发达地区的学习和研究，减少城乡之间的教育差异。

-   **提高社会意识和参与**：通过提供高效的信息检索工具，可以增强公众对于重要社会问题的意识和理解，比如环境保护、公共健康和社会正义等。用户可以更容易地获取相关信息，从而在这些重要问题上做出更为明智的决策和参与。

**技术与可持续性的结合**

-   在技术实现方面，本项目特别重视环保和可持续性原则。通过采用最新的算法优化技术，我们显著提高了数据处理效率，从而减少了能耗。同时，项目在选择服务器和存储解决方案时，优先考虑那些采用可再生能源的服务提供商。这种策略不仅降低了系统的环境足迹，也体现了我们对环境保护的承诺。

**促进包容性和平等**

-   信息检索系统设计之初就考虑到了多样性和包容性，特别是在语言处理功能上。系统支持中英文的自然语言处理，确保了不同语言用户的信息获取需求得到满足。此外，我们通过用户界面的多语言支持和无障碍设计，使系统对不同文化和能力水平的用户都友好，从而推动了信息获取的平等性和包容性。

## 实验总结

<!-- todo 重新生成一个总结 -->

本次实验的核心目标是设计并实现一个多功能的信息检索系统，该系统能够处理中英文文档的检索，并提供准确的搜索结果。实验过程中，我们面临了各种挑战，同时也获得了宝贵的知识与经验。

1. **数据爬取与处理：** 我们使用了 Scrapy 框架来爬取网络数据，并存储为 json 格式。处理数据时，需要对中文进行分词，这一步骤对准确建立倒排索引至关重要。我们选择使用结巴分词框架，它在处理中文分词时表现出色，但同时也需要调整参数以适应不同的语境与专业术语。
2. **倒排索引与搜索算法的实现：** 建立高效的倒排索引并实现基于 TF-IDF 的向量空间模型匹配算法是另一个挑战。这要求我们不仅要在理论上掌握信息检索的基本原理，还要在实践中调整和优化算法。
3. **多媒体检索服务：** 利用 Tensorflow 和 Flask 框架开发图片检索功能，我们需要处理大量的图像数据，并从中提取关键词进行检索。这一过程中的图像识别与关键词提取对计算资源和算法优化提出了更高的要求。

通过本项目，我们深入学习了信息检索的核心技术和算法。在实际编码和实现过程中，我们对 Scrapy 和结巴分词的应用有了更深入的理解。同时，通过实际操作 Tensorflow 框架，我们增强了对机器学习在图像处理应用中的理解和技能。此外，项目的多语言处理能力也让我们了解到语言处理的复杂性和技术的多样性。

本信息检索系统成功实现了从数据爬取到用户界面的全流程服务。系统能够有效地对输入的查询进行处理，并按照相关度返回准确的搜索结果。特别是在中文文档的处理上，通过优化分词和索引构建过程，提高了检索的准确率和效率。在项目展示和评审中，该系统得到了用户和专家的肯定，验证了我们设计和实现的有效性。总之，这次实验不仅提升了我们的技术能力，也加深了我们对信息检索领域的理解。

## 实验分工

|      | 郭晨旭                                                                 | 韩景锐                                                         |
| ---- | ---------------------------------------------------------------------- | -------------------------------------------------------------- |
| 学号 | 2021211184                                                             | 2021211176                                                     |
| 代码 | Go 后端，Scrapy 爬虫，Python 图片提取关键词服务，Python 信息抽取服务   | Vue 前端                                                       |
| 报告 | 项目概述，后端设计与实现，信息抽取服务，多媒体信息抽取服务，优化与创新 | 前端设计与实现，优化与创新，环境和社会可持续发展思考，实验总结 |
